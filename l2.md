## Cache review
### Register file
- Array (Square)
- Raw decoder, column decoder

### SRAM
- Memory array
- Many transiters for a cell in the memory square

### DRAM
- Memory array
- Structure is different from SRAM
- Take the smallest area to store a bit
- One transistor and on capacitor
- Trade off between capacity, latency, bandwith, etc.

### CPU-Memory bottleneck
1. latency : time for a single access
2. bandwidth : number of access per unit time
3. bandwidth-delay product : the amount of data that can be in flight at the same time

- physical size affect latency (data longer to traval)

### Memory hierarchy
- latency : on-chip << off-chip. That's why we need fast memory (RF, SRAM)
- The hierarchy only works when the reused data is in the fast memory, or it is just waste of energy.
- Memory access pattern, for instruction, stack and data. There are some patterns that we can exploit.
    - Temporal locality
    - Spacial locality
    - Cache can exploit both of the locality

### Cache
- Index : which row
- Tag : which one in a set. The larger the associativity, more bits are needed for tag
- Offset : which data location
- cache hit
    - write throught : write both cache and memory
    - write back : write only to cache, memory is written when evicted (issue about dirty bit)
- cache miss
    - no write allocate : only wirte to main memory
    - write allocate : fetch to cache
    - compulsory : first reference
    - capacity : how many data can be put in
    - conflict : two instruction has the same hash value for the cache 
- reduce hit time : small cache is better
- reduce miss rate
    - larger block size (but not always increase with the size, because the larger block size cause conflict more easily)
    - larger cache size
    - high associativity (why?)

### Superscalar
- Data hazards
    - WAR
    - RAW
    - WAW
- CPI can be <= 1 (multiple instructions in parallel)
- issue about fetch cross cache line (fetch and alignment contraint)
